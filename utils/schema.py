"""
Pydantic schemas for structured data used throughout the agentic RAG pipeline.

This module defines the data structures for:
- Individual steps in an action plan.
- The overall action plan.
- Retrieved document chunks.
- Verification results.
- Final aggregated answers.

These schemas ensure that data passed between components (Planner, Executor, Verifier, etc.)
is well-defined, validated, and easy to serialize/deserialize.
"""
from typing import List, Dict, Any, Literal, Optional
from pydantic import BaseModel, Field

class ActionStep(BaseModel):
    """
    Represents a single step in the execution plan generated by the Planner.
    """
    id: int = Field(..., description="A unique identifier for the step, starting from 1.")
    action: str = Field(..., description="The high-level action to be performed (e.g., 'retrieve', 'summarize', 'verify').")
    tool: str = Field(..., description="The specific tool or component to use for this action (e.g., 'hybrid_retriever', 'llm_summarizer', 'web_search').")
    input: Dict[str, Any] = Field(..., description="A dictionary of parameters to pass to the tool's execution method.")
    expected_output_schema: str = Field(..., description="A string describing the expected structure or type of the output, for validation.")

class ActionPlan(BaseModel):
    """
    A structured plan consisting of a sequence of action steps.
    """
    query: str = Field(..., description="The original user query.")
    plan: List[ActionStep] = Field(..., description="A list of action steps to execute in sequence.")
    reasoning: str = Field(..., description="The reasoning behind the generated plan.")

class DocumentChunk(BaseModel):
    """
    Represents a piece of retrieved information from a data source.
    """
    source: str = Field(..., description="The origin of the document (e.g., file path, URL, database table).")
    content: str = Field(..., description="The text content of the chunk.")
    score: float = Field(..., description="The relevance score from the retrieval process.")
    metadata: Dict[str, Any] = Field({}, description="Additional metadata associated with the chunk.")

class VerificationResult(BaseModel):
    """
    The result of a fact-checking or verification step.
    """
    verdict: Literal["SUPPORTS", "CONTRADICTS", "INSUFFICIENT_EVIDENCE"] = Field(..., description="The outcome of the verification.")
    evidence: List[DocumentChunk] = Field(..., description="A list of document chunks that support the verdict.")
    reasoning: str = Field(..., description="A brief explanation of why the verdict was reached.")

class FinalAnswer(BaseModel):
    """
    The final, aggregated answer presented to the user.
    """
    answer: str = Field(..., description="The synthesized, final answer to the user's query.")
    sources: List[DocumentChunk] = Field(..., description="A list of the primary sources used to construct the answer.")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="A score from 0.0 to 1.0 indicating the system's confidence in the answer.")
    unverified_claims: List[str] = Field([], description="Any claims that could not be verified during the process.")

# TODO: Add more specific schemas for tool inputs and outputs as tools are implemented.
# For example, a schema for the web_search tool's output might include a URL, title, and snippet.
